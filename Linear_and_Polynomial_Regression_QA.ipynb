{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262c5bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install matplotlib scikit-learn seaborn --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f0587a",
   "metadata": {},
   "source": [
    "## 1. What is Simple Linear Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5108cfb1",
   "metadata": {},
   "source": [
    "Simple Linear Regression is a statistical method used to model the relationship between a single independent variable (X) and a dependent variable (Y) by fitting a linear equation to observed data. The model is of the form:\n",
    "\n",
    "\\[ Y = mX + c \\]\n",
    "\n",
    "Where:\n",
    "- \\( Y \\) is the predicted value,\n",
    "- \\( m \\) is the slope of the line,\n",
    "- \\( X \\) is the independent variable,\n",
    "- \\( c \\) is the intercept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f5844e",
   "metadata": {},
   "source": [
    "## 2. What are the key assumptions of Simple Linear Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da20daca",
   "metadata": {},
   "source": [
    "1. Linearity\n",
    "2. Independence of errors\n",
    "3. Homoscedasticity (constant variance of errors)\n",
    "4. Normality of errors\n",
    "5. No multicollinearity (though multicollinearity is less of an issue in simple regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967e75b9",
   "metadata": {},
   "source": [
    "## 3. What does the coefficient m represent in the equation Y = mX + c?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bef186",
   "metadata": {},
   "source": [
    "The coefficient \\( m \\) represents the **slope** of the regression line. It indicates the change in the dependent variable \\( Y \\) for a one-unit change in the independent variable \\( X \\)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f1ee56",
   "metadata": {},
   "source": [
    "## 4. What does the intercept c represent in the equation Y = mX + c?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2514e42e",
   "metadata": {},
   "source": [
    "The intercept \\( c \\) is the predicted value of \\( Y \\) when \\( X = 0 \\). It represents the point where the regression line crosses the Y-axis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58611cce",
   "metadata": {},
   "source": [
    "## 5. How do we calculate the slope m in Simple Linear Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89bdeb0",
   "metadata": {},
   "source": [
    "The formula for slope \\( m \\):\n",
    "\n",
    "\\[ m = \\frac{n\\sum xy - \\sum x \\sum y}{n \\sum x^2 - (\\sum x)^2} \\]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9338db0",
   "metadata": {},
   "source": [
    "## 6. What is the purpose of the least squares method in Simple Linear Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78254f2",
   "metadata": {},
   "source": [
    "The **least squares method** minimizes the sum of squared differences between the observed and predicted values. It helps find the best-fitting regression line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28fad29",
   "metadata": {},
   "source": [
    "## 7. How is the coefficient of determination (R²) interpreted in Simple Linear Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc17e109",
   "metadata": {},
   "source": [
    "R² represents the proportion of the variance in the dependent variable that is predictable from the independent variable. It ranges from 0 to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03eb3cf9",
   "metadata": {},
   "source": [
    "## 8. What is Multiple Linear Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f396a939",
   "metadata": {},
   "source": [
    "Multiple Linear Regression is a method that models the relationship between a dependent variable and **two or more** independent variables.\n",
    "\n",
    "\\[ Y = b_0 + b_1X_1 + b_2X_2 + ... + b_nX_n \\]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6c95de",
   "metadata": {},
   "source": [
    "## 9. What is the main difference between Simple and Multiple Linear Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8954b28",
   "metadata": {},
   "source": [
    "- Simple Linear Regression: one independent variable\n",
    "- Multiple Linear Regression: two or more independent variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc16d9df",
   "metadata": {},
   "source": [
    "## 10. What are the key assumptions of Multiple Linear Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fbdcf0",
   "metadata": {},
   "source": [
    "1. Linearity\n",
    "2. Independence of errors\n",
    "3. Homoscedasticity\n",
    "4. Normality of errors\n",
    "5. No multicollinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db43122",
   "metadata": {},
   "source": [
    "## 11. What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae5eac3",
   "metadata": {},
   "source": [
    "Heteroscedasticity refers to non-constant variance of residuals. It affects the model by:\n",
    "- Invalidating statistical tests\n",
    "- Reducing prediction accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bacc4c",
   "metadata": {},
   "source": [
    "## 12. How can you improve a Multiple Linear Regression model with high multicollinearity?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00949c57",
   "metadata": {},
   "source": [
    "- Remove correlated predictors\n",
    "- Use PCA (Principal Component Analysis)\n",
    "- Apply Ridge or Lasso Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4459d8",
   "metadata": {},
   "source": [
    "## 13. What are some common techniques for transforming categorical variables for use in regression models?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79136032",
   "metadata": {},
   "source": [
    "- One-hot encoding\n",
    "- Label encoding\n",
    "- Binary encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0946bc5f",
   "metadata": {},
   "source": [
    "## 14. What is the role of interaction terms in Multiple Linear Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4359bcb1",
   "metadata": {},
   "source": [
    "Interaction terms allow modeling of combined effects of two or more variables on the target variable. It captures non-additive relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b41e4d",
   "metadata": {},
   "source": [
    "## 15. How can the interpretation of intercept differ between Simple and Multiple Linear Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76586fc",
   "metadata": {},
   "source": [
    "- Simple: Y-intercept when X = 0\n",
    "- Multiple: Value of Y when **all Xs = 0**, which may or may not be meaningful"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13852f52",
   "metadata": {},
   "source": [
    "## 16. What is the significance of the slope in regression analysis, and how does it affect predictions?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0140d99",
   "metadata": {},
   "source": [
    "The slope indicates the rate of change of the dependent variable with respect to the independent variable. A higher slope indicates a stronger effect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cfbaeb",
   "metadata": {},
   "source": [
    "## 17. How does the intercept in a regression model provide context for the relationship between variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4983c329",
   "metadata": {},
   "source": [
    "It provides a baseline value of the dependent variable when all independent variables are 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8548ed",
   "metadata": {},
   "source": [
    "## 18. What are the limitations of using R² as a sole measure of model performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbb9ed0",
   "metadata": {},
   "source": [
    "- Doesn't indicate whether the model is biased\n",
    "- Increases with more variables (overfitting)\n",
    "- Doesn’t indicate causal relationship"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b2495f",
   "metadata": {},
   "source": [
    "## 19. How would you interpret a large standard error for a regression coefficient?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df12bc7",
   "metadata": {},
   "source": [
    "A large standard error indicates that the coefficient is not reliably estimated, suggesting a lack of significance or high variability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe9623b",
   "metadata": {},
   "source": [
    "## 20. How can heteroscedasticity be identified in residual plots, and why is it important to address it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855d023e",
   "metadata": {},
   "source": [
    "- Identified when residuals fan out or form patterns.\n",
    "- It violates model assumptions and affects the validity of confidence intervals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddae7001",
   "metadata": {},
   "source": [
    "## 21. What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffb99f8",
   "metadata": {},
   "source": [
    "It means the added variables do not contribute significantly to the model. Adjusted R² penalizes unnecessary variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d881ed6",
   "metadata": {},
   "source": [
    "## 22. Why is it important to scale variables in Multiple Linear Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac37a94",
   "metadata": {},
   "source": [
    "- Prevents dominance of larger scale variables\n",
    "- Important for regularized models like Ridge, Lasso\n",
    "- Ensures model convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d669c201",
   "metadata": {},
   "source": [
    "## 23. What is polynomial regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb0e0e0",
   "metadata": {},
   "source": [
    "Polynomial regression is a form of regression analysis in which the relationship between the independent and dependent variables is modeled as an nth-degree polynomial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f79de0f",
   "metadata": {},
   "source": [
    "## 24. How does polynomial regression differ from linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a893a05",
   "metadata": {},
   "source": [
    "- Linear regression fits a straight line.\n",
    "- Polynomial regression fits a curved line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793a6646",
   "metadata": {},
   "source": [
    "## 25. When is polynomial regression used?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2435fe",
   "metadata": {},
   "source": [
    "- When the relationship between the variables is non-linear.\n",
    "- Useful in modeling curves or trends."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729ea1a3",
   "metadata": {},
   "source": [
    "## 26. What is the general equation for polynomial regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d76ca94",
   "metadata": {},
   "source": [
    "\\[ Y = b_0 + b_1X + b_2X^2 + b_3X^3 + ... + b_nX^n \\]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46ea036",
   "metadata": {},
   "source": [
    "## 27. Can polynomial regression be applied to multiple variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32a3c1f",
   "metadata": {},
   "source": [
    "Yes, it becomes a **multivariate polynomial regression**, where each variable can be raised to different powers and combined."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b380fd",
   "metadata": {},
   "source": [
    "## 28. What are the limitations of polynomial regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deee83ac",
   "metadata": {},
   "source": [
    "- Overfitting with high degrees\n",
    "- Difficult to interpret\n",
    "- Sensitive to outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dba28a",
   "metadata": {},
   "source": [
    "## 29. What methods can be used to evaluate model fit when selecting the degree of a polynomial?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e50c4f9",
   "metadata": {},
   "source": [
    "- Cross-validation\n",
    "- Adjusted R²\n",
    "- AIC/BIC\n",
    "- Visual inspection of residual plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06ebd6f",
   "metadata": {},
   "source": [
    "## 30. Why is visualization important in polynomial regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7d1e46",
   "metadata": {},
   "source": [
    "- Helps detect overfitting or underfitting\n",
    "- Makes model behavior interpretable\n",
    "- Useful in selecting the appropriate polynomial degree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c540825",
   "metadata": {},
   "source": [
    "## 31. How is polynomial regression implemented in Python?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3ca6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample data\n",
    "X = np.arange(1, 11).reshape(-1, 1)\n",
    "y = np.array([1.2, 1.9, 3.1, 3.9, 5.1, 6.0, 6.8, 8.3, 9.1, 9.9])\n",
    "\n",
    "# Degree 2 polynomial\n",
    "model = make_pipeline(PolynomialFeatures(degree=2), LinearRegression())\n",
    "model.fit(X, y)\n",
    "\n",
    "# Predictions\n",
    "X_test = np.linspace(1, 10, 100).reshape(-1, 1)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Plotting\n",
    "plt.scatter(X, y, color='red', label='Data')\n",
    "plt.plot(X_test, y_pred, color='blue', label='Polynomial Fit')\n",
    "plt.title('Polynomial Regression (degree=2)')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}